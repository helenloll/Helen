{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular dataset with pipleine - light version to share , 19.11.2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Datastore, Dataset, Workspace, Experiment, RunConfiguration\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "import os\n",
    "azureml.core.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write workspace to file\n",
    "from azureml.core import Workspace\n",
    "\n",
    "subscription_id = '8b2f4    '\n",
    "resource_group  = 'HelenMachineLearning'\n",
    "workspace_name  = 'HelenMachineLearning'\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My info\n",
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, datastore.name, sep = '\\n')\n",
    "\n",
    "helen_datastore = Datastore.get(workspace=ws, datastore_name='helen_blobstore')\n",
    "helen_datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attache Azure ML Compute as Cluster of low cost nodes\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"automl-compute\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes,\n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(\n",
    "        ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datastore - registering at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING DATASTORE\n",
    "\n",
    "from azureml.core import Workspace, Experiment, Datastore, Dataset\n",
    "\n",
    "blob_datastore_name='helen_blobstore' # Name of the datastore to workspace\n",
    "container_name=os.getenv(\"BLOB_CONTAINER\", \"helenml\") # Name of Azure blob container\n",
    "account_name=os.getenv(\"BLOB_ACCOUNTNAME\", \"storagehelen\") # Storage account name\n",
    "account_key=os.getenv(\"BLOB_ACCOUNT_KEY\", \"as== \") # Storage account key\n",
    "\n",
    "helen_datastore = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                         datastore_name=blob_datastore_name, \n",
    "                                                         container_name=container_name, \n",
    "                                                         account_name=account_name,\n",
    "                                                         account_key=account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all datastores registered in the current workspace\n",
    "datastores = ws.datastores\n",
    "for name, datastore in datastores.items():\n",
    "    print (name, datastore.datastore_type, datastore.account_name, datastore.container_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular dataset - registering at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder=\"./helen/data/\"\n",
    "os.listdir(script_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading data files once\n",
    "helen_datastore = Datastore.get(workspace=ws, datastore_name='helen_blobstore')\n",
    "\n",
    "\n",
    "\n",
    "helen_datastore.upload_files(files = ['./helen/data/diabetes_data.txt'],\n",
    "                       target_path = '/helen/data',\n",
    "                       overwrite = True,\n",
    "                       show_progress = True)\n",
    "\n",
    "\n",
    "helen_datastore.upload_files(files = ['./helen/data/diabetes_labels.txt'],\n",
    "                       target_path = '/helen/data',\n",
    "                       overwrite = True,\n",
    "                       show_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering Tabular data ONCE\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "helen_datastore = Datastore.get(workspace=ws, datastore_name='helen_blobstore')\n",
    "\n",
    "##########################\n",
    "#diabetes data\n",
    "##############\n",
    "diabetes_data = Dataset.Tabular.from_delimited_files(path=[(helen_datastore, '/helen/data/diabetes_data.txt')],separator=' ')\n",
    "diabetes_data = diabetes_data.register(workspace=ws, \n",
    "                                 name='diabetes_data',\n",
    "                                 description='diabetes data',\n",
    "                                 create_new_version=True)\n",
    "\n",
    "\n",
    "##########################\n",
    "#diabetes labels\n",
    "##############\n",
    "diabetes_labels = Dataset.Tabular.from_delimited_files(path=[(helen_datastore, '/helen/data/diabetes_labels.txt')],separator=' ')\n",
    "diabetes_labels = diabetes_labels.register(workspace=ws,\n",
    "                                 name='diabetes_labels',\n",
    "                                 description='diabetes labels',\n",
    "                                 create_new_version=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular data set - accessing it in scipt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing dataset which is already registered\n",
    "# get dataset by dataset name\n",
    "diabetes_data = Dataset.get_by_name(workspace=ws, name='diabetes_data')\n",
    "\n",
    "df = diabetes_data.to_pandas_dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing dataset which is already registered\n",
    "# get dataset by dataset name\n",
    "diabetes_labels = Dataset.get_by_name(workspace=ws, name='diabetes_labels')\n",
    "\n",
    "df = diabetes_labels.to_pandas_dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory in my local comuter\n",
    "script_folder = './helen/script'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "os.listdir(script_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./helen/script/diabetes_prep.py\n",
    "\n",
    "# simple read and train\n",
    "\n",
    "from azureml.core import Dataset, Run\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from matplotlib import pyplot as plot\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "# AML content - start\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "print ('HELEN PREP STEP ')\n",
    "output_dir='./helen/output'\n",
    "os.makedirs ('./helen/output',exist_ok=True)\n",
    "run = Run.get_context()\n",
    "\n",
    "##########################################\n",
    "# get the input dataset by name\n",
    "##########################################\n",
    "\n",
    "dataset = run.input_datasets['diabetes_data']\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "df = dataset.to_pandas_dataframe()\n",
    "x_array=df.to_numpy()\n",
    "\n",
    "\n",
    "dataset = run.input_datasets['diabetes_labels']\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "df = dataset.to_pandas_dataframe()\n",
    "y_array=df.to_numpy()\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "\n",
    "run.log('data cnt',df.count())\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "# AML content - end\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "\n",
    "# My regural python code\n",
    "y=y_array\n",
    "X=x_array\n",
    "columns = ['age', 'gender', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "data = {\n",
    "   \"train\":{\"X\": X_train, \"y\": y_train},        \n",
    "   \"test\":{\"X\": X_test, \"y\": y_test}\n",
    "}\n",
    "reg = Ridge(alpha = 0.03)\n",
    "reg.fit(data['train']['X'], data['train']['y'])\n",
    "preds = reg.predict(data['test']['X'])\n",
    "print('Mean Squared Error is', mean_squared_error(preds, data['test']['y']))\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "# AML content - start\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "# Log mse in Azure ML logs\n",
    "run.log('mse', mean_squared_error(preds, data['test']['y']))\n",
    "\n",
    "# Save the model to the outputs directory for capture\n",
    "model_file = 'diabetes_helen.pkl'\n",
    "model_file_name=os.path.join(output_dir, model_file)\n",
    "joblib.dump(value = reg, filename = model_file_name);\n",
    "print(run.get_file_names())\n",
    "\n",
    "# upload the model file explicitly into artifacts Azure ML artifacts\n",
    "run.upload_file(name = model_file_name, path_or_stream = model_file_name)\n",
    "\n",
    "# register model in Azure ML Resitry \n",
    "model = run.register_model(model_name='helen_test',model_path=model_file_name)\n",
    "print(model.name, model.id, model.version, sep='\\t')\n",
    "\n",
    "for a in range (len(preds)):\n",
    "    run.log_row(\"Error: Estimate  - Actual\", x=a, y=abs (float (preds[a]) - float(y_test[a])))\n",
    "    \n",
    "\n",
    "# Creating file to oputput\n",
    "num_rows, num_cols = X_test.shape\n",
    "pred = preds.reshape((num_rows, 1))\n",
    "actual=y_test.reshape((num_rows, 1))\n",
    "\n",
    "tmp_npy = np.append (X_test, actual, 1)\n",
    "helen_numpy = np.append (tmp_npy, pred, 1)\n",
    "print ('helen_numpy shape ',helen_numpy.shape)\n",
    "\n",
    "helen_pandas=pd.DataFrame(data=helen_numpy)\n",
    "\n",
    "LOCALFILENAME='helen_score_file.txt'\n",
    "score_dir='./logs'\n",
    "score_dir='./helen/score'\n",
    "\n",
    "# Uploading file as articraft\n",
    "os.makedirs (score_dir,exist_ok=True)\n",
    "score_file = os.path.join(score_dir, LOCALFILENAME) \n",
    "helen_pandas.to_csv(score_file, sep=',', encoding='utf-8', index=False)\n",
    "print ('file name', score_file)\n",
    "\n",
    "# upload scored data explicitly into artifacts \n",
    "run.upload_file(name = score_file, path_or_stream = score_file)\n",
    "\n",
    "##########################################\n",
    "# create output refernce for dataset in pipeline step\n",
    "##########################################\n",
    "mounted_output_path = os.environ['AZUREML_DATAREFERENCE_diabetes_temp_ds']\n",
    "os.makedirs(mounted_output_path, exist_ok=True)\n",
    "score_file = os.path.join(mounted_output_path, LOCALFILENAME) \n",
    "helen_pandas.to_csv(score_file, sep=',', encoding='utf-8', index=False)\n",
    "print ('file name to somewhere', score_file)\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "# AML content - end\n",
    "##########################################\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./helen/script/diabetes_train.py\n",
    "\n",
    "\n",
    "# simple prep and train\n",
    "from azureml.core import Dataset, Run\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from matplotlib import pyplot as plot\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "# AML content - start\n",
    "##########################################\n",
    "##########################################\n",
    "print ('HELEN TRAIN STEP ')\n",
    "\n",
    "output_dir='./helen/output'\n",
    "os.makedirs ('./helen/output',exist_ok=True)\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "#####################################\n",
    "# get the input dataset by name\n",
    "#####################################\n",
    "dataset = run.input_datasets['diabetes_data']\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "df = dataset.to_pandas_dataframe()\n",
    "x_array=df.to_numpy()\n",
    "\n",
    "\n",
    "dataset = run.input_datasets['diabetes_labels']\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "df = dataset.to_pandas_dataframe()\n",
    "y_array=df.to_numpy()\n",
    "\n",
    "\n",
    "dataset = run.input_datasets['diabetes_temp_ds']\n",
    "# load dataset into pandas dataframe\n",
    "df = dataset.to_pandas_dataframe()\n",
    "xy_array = df.to_numpy()\n",
    "#####################################\n",
    "# get the input dataset by name\n",
    "#####################################\n",
    "\n",
    "run.log('data cnt',df.count())\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "# AML content - end\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "\n",
    "\n",
    "# my regular python code\n",
    "y=y_array\n",
    "X=x_array\n",
    "columns = ['age', 'gender', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "data = {\n",
    "   \"train\":{\"X\": X_train, \"y\": y_train},        \n",
    "   \"test\":{\"X\": X_test, \"y\": y_test}\n",
    "}\n",
    "reg = Ridge(alpha = 0.03)\n",
    "reg.fit(data['train']['X'], data['train']['y'])\n",
    "preds = reg.predict(data['test']['X'])\n",
    "print('Mean Squared Error is', mean_squared_error(preds, data['test']['y']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "# AML content - start\n",
    "##########################################\n",
    "##########################################\n",
    "# Log mse to Azure ML\n",
    "run.log('mse', mean_squared_error(preds, data['test']['y']))\n",
    "\n",
    "# Save the model to the outputs directory for capture\n",
    "model_file = 'diabetes_helen.pkl'\n",
    "model_file_name=os.path.join(output_dir, model_file)\n",
    "joblib.dump(value = reg, filename = model_file_name);\n",
    "print(run.get_file_names())\n",
    "\n",
    "# upload the model file explicitly into artifacts in Azure ML\n",
    "run.upload_file(name = model_file_name, path_or_stream = model_file_name)\n",
    "\n",
    "# register model in Azure ML\n",
    "model = run.register_model(model_name='helen_test',model_path=model_file_name)\n",
    "print(model.name, model.id, model.version, sep='\\t')\n",
    "\n",
    "# Log in Azure ML\n",
    "for a in range (len(preds)):\n",
    "    run.log_row(\"Error: Estimate  - Actual\", x=a, y=abs (float (preds[a]) - float(y_test[a])))\n",
    "    \n",
    "# Logging histogram plot in Azue ML \n",
    "num_rows, num_cols = X_test.shape\n",
    "pred = preds.reshape((num_rows, 1))\n",
    "actual=y_test.reshape((num_rows, 1))\n",
    "tmp_npy = np.append (X_test, actual, 1)\n",
    "helen_numpy = np.append (tmp_npy, pred, 1)\n",
    "\n",
    "\n",
    "f=helen_numpy\n",
    "print (f.shape)\n",
    "fnrow=f.shape[0]\n",
    "fncol=f.shape[1]\n",
    "print ( \" rows \", fnrow, \"columns \", fncol)\n",
    "\n",
    "# Histograms to all columns\n",
    "i=0\n",
    "for i in range (fncol):\n",
    "    title= str (i) + ' nr column  '\n",
    "    plot.title(title)\n",
    "    plot.hist (f[:,[i]],bins=30,color='blue',edgecolor='white')\n",
    "    #CORRECTplot.show()\n",
    "    run.log_image ('Helen plot_' + str (i),plot=plot)\n",
    "    plot.clf()\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "# AML content - end\n",
    "##########################################\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating - Dataset to be used between pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define intermediate data\n",
    "helen_datastore = Datastore.get(workspace=ws, datastore_name='helen_blobstore')\n",
    "\n",
    "diabetes_temp_ds = PipelineData('diabetes_temp_ds', datastore=helen_datastore).as_dataset()\n",
    "\n",
    "# register output data as dataset\n",
    "diabetes_temp_ds= diabetes_temp_ds.register(name='diabetes_temp_ds', create_new_version=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating - Python step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# python script configuration\n",
    "\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a new runconfig object\n",
    "aml_run_config = RunConfiguration()\n",
    "\n",
    "# Use the aml_compute you created above. \n",
    "aml_run_config.target = compute_target\n",
    "\n",
    "# Enable Docker\n",
    "aml_run_config.environment.docker.enabled = True\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=['pandas','scikit-learn','matplotlib'], \n",
    "    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]'], \n",
    "    pin_sdk_version=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python step\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "\n",
    "helen_prep_step1 = PythonScriptStep(name='diabetes_prep',\n",
    "                             script_name=\"diabetes_prep.py\",\n",
    "                             inputs=[diabetes_data.as_named_input('diabetes_data'),diabetes_labels.as_named_input('diabetes_labels')],\n",
    "                             #CORRECT outputs=[diabetes_temp_ds.as_named_input('diabetes_temp_ds')],\n",
    "                             outputs=[diabetes_temp_ds],\n",
    "                             source_directory=script_folder,\n",
    "                             compute_target=compute_target,\n",
    "                             runconfig=aml_run_config,\n",
    "                             allow_reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating pipeline - with one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pipeline with one step\n",
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[helen_prep_step1])\n",
    "\n",
    "pipeline_run = Experiment(ws, 'helen_1steps_pipeline').submit(pipeline)\n",
    "\n",
    "# this will output a table with link to the run details in azure portal\n",
    "pipeline_run\n",
    "#Console logs\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating - estimator SKLearn step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION FOR TRAINING\n",
    "# Azure ML will create for me docker image \n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "data_dir='./helen/data'\n",
    "script_dir='./helen/script'\n",
    "\n",
    "est = SKLearn(source_directory=script_dir,\n",
    "                entry_script='diabetes_train.py',\n",
    "                pip_packages = ['azureml-sdk','azureml-dataprep[fuse,pandas]','matplotlib'],\n",
    "                compute_target=compute_target\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training step \n",
    "helen_train_step = EstimatorStep(name='diabates_train',\n",
    "                         estimator=est,\n",
    "                         estimator_entry_script_arguments=[],\n",
    "                         # parse prepared_fashion_ds into TabularDataset and use it as the input\n",
    "                         inputs=[diabetes_temp_ds.parse_delimited_files(), diabetes_data.as_named_input('diabetes_data'),diabetes_labels.as_named_input('diabetes_labels')],\n",
    "                         compute_target=compute_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating pipeline  - with two steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline & run experiment\n",
    "# run pipeline \n",
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[helen_prep_step1,helen_train_step])\n",
    "\n",
    "pipeline_run = Experiment(ws, 'diabetes_pipeline').submit(pipeline)\n",
    "\n",
    "# this will output a table with link to the run details in azure portal\n",
    "pipeline_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUI\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Console logs\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
