{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support notebook working with Azure ML\n",
    "\n",
    "This notebook contains support code working with Azure ML\n",
    "- working workspace\n",
    "- training with data\n",
    "- scoring \n",
    "- run logs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main library\n",
    "import azureml.core\n",
    "from azureml.core import Dataset, Model , Workspace, Experiment\n",
    "import os\n",
    "\n",
    "#import azureml.contrib.dataset\n",
    "\n",
    "azureml.core.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write workspace to file\n",
    "from azureml.core import Workspace\n",
    "\n",
    "subscription_id = 'df6'\n",
    "resource_group  = 'Helen_MachineLearning'\n",
    "workspace_name  = 'Helen_MachineLearning'\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read current workspace from file\n",
    "ws=Workspace.from_config()\n",
    "print (ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List experiments\n",
    "list_experiments = Experiment.list(ws)\n",
    "print (list_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List runs under experiment \n",
    "experiment = Experiment(workspace=ws, name=\"therealbank_training\")\n",
    "print (experiment)\n",
    "list_runs = experiment.get_runs()\n",
    "for run in list_runs:\n",
    "    print(run.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archiving experiment\n",
    "experiment = Experiment(workspace=ws, name=\"therealbank_training\")\n",
    "experiment.archive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list datasets\n",
    "Dataset.get_all(workspace=ws)\n",
    "list_datasets= Dataset.list(workspace=ws)\n",
    "for ds in list_datasets:\n",
    "    print(ds.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch my dataset (to pandas)\n",
    "dataset = Dataset.get_by_name(workspace=ws, name='therealbank')\n",
    "df = dataset.to_pandas_dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all my models\n",
    "\n",
    "list_d= Model.list(workspace=ws)\n",
    "for ds in list_d:\n",
    "    print(ds.name, ds.version)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List specific model and delete it\n",
    "list_d= Model.list(workspace=ws,name='AutoMLaf4c0e6eb68')\n",
    "for ds in list_d:\n",
    "    print(ds.name, ds.version)\n",
    "    ds.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List specific model version and delete it\n",
    "modelname='helen_test'\n",
    "model = Model(ws, modelname,version=1)\n",
    "model.delete()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach compute \n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"automl-compute\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes,\n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(\n",
    "        ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Current compute \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print my default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "print (datastore)\n",
    "print (datastore.datastore_type, datastore.account_name, datastore.container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all datastores registered in the current workspace\n",
    "datastores = ws.datastores\n",
    "for name, datastore in datastores.items():\n",
    "    print (name, datastore.datastore_type, datastore.account_name, datastore.container_name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DATASTORE\n",
    "from azureml.core import Workspace, Experiment, Datastore, Dataset\n",
    "blob_datastore_name='helen_blobstore' # Name of the datastore to workspace\n",
    "container_name=os.getenv(\"BLOB_CONTAINER\", \"helenml\") # Name of Azure blob container\n",
    "account_name=os.getenv(\"BLOB_ACCOUNTNAME\", \"storagehelen\") # Storage account name\n",
    "account_key=os.getenv(\"BLOB_ACCOUNT_KEY\", \"jecd6 ....\") # Storage account key\n",
    "\n",
    "helen_datastore = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                         datastore_name=blob_datastore_name, \n",
    "                                                         container_name=container_name, \n",
    "                                                         account_name=account_name,\n",
    "                                                         account_key=account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading data to Datastore \n",
    "helen_datastore = Datastore.get(workspace=ws, datastore_name='helen_blobstore')\n",
    "\n",
    "\n",
    "\n",
    "helen_datastore.upload_files(files = ['./diabetes/data/diabetes_data.txt'],\n",
    "                       target_path = '/helen/data',\n",
    "                       overwrite = True,\n",
    "                       show_progress = True)\n",
    "\n",
    "helen_datastore.upload_files(files = ['./diabetes/data/diabetes_labels.txt'],\n",
    "                       target_path = '/helen/data',\n",
    "                       overwrite = True,\n",
    "                       show_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach specific datastore\n",
    "helen_datastore = Datastore.get(workspace=ws, datastore_name='helen_blobstore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all datastores registered in the current workspace\n",
    "datastores = ws.datastores\n",
    "for name, datastore in datastores.items():\n",
    "    #print(name, datastore.datastore_type)\n",
    "    print (name, datastore.datastore_type, datastore.account_name, datastore.container_name)\n",
    "    #print(datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download - download creates folders directly into root, or reuses which are already there\n",
    "datastore.download(target_path= './',\n",
    "                   prefix='helen/data/',\n",
    "                   show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datasets \n",
    "data_dir='./helen/data'\n",
    "\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "##########################\n",
    "#diabetes data\n",
    "##############\n",
    "diabetes_data = Dataset.Tabular.from_delimited_files(path=[(datastore, '/helen/data/diabetes_data.txt')],separator=' ')\n",
    "\n",
    "diabetes_data = diabetes_data.register(workspace=ws,\n",
    "                                 name='diabetes_data',\n",
    "                                 description='diabetes data',\n",
    "                                      create_new_version=True)\n",
    "\n",
    "\n",
    "##########################\n",
    "#diabetes labels\n",
    "##############\n",
    "diabetes_labels = Dataset.Tabular.from_delimited_files(path=[(datastore, '/helen/data/diabetes_labels.txt')],separator=' ')\n",
    "\n",
    "diabetes_labels = diabetes_labels.register(workspace=ws,\n",
    "                                 name='diabetes_labels',\n",
    "                                 description='diabetes labels',\n",
    "                                      create_new_version=True)\n",
    "\n",
    "\n",
    "##########################\n",
    "# diabates al las file\n",
    "# create a FileDataset pointing to files in folder and its subfolders recursively\n",
    "#datastore_paths = [(datastore, 'helen/data')]\n",
    "##########################\n",
    "diabetes_file = Dataset.File.from_files(path=[(datastore,'/helen/data/diabetes_data.txt')])\n",
    "\n",
    "diabetes_file = diabetes_file.register(workspace=ws,\n",
    "                                 name='diabetes_data_file',\n",
    "                                 description='diabetes data file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing dataset which is already registered\n",
    "# get dataset by dataset name\n",
    "diabetes_data = Dataset.get_by_name(workspace=ws, name='diabetes_data')\n",
    "diabetes_labels = Dataset.get_by_name(workspace=ws, name='diabetes_labels')\n",
    "\n",
    "df = diabetes_data.to_pandas_dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory in my local comuter\n",
    "script_folder = './helen/script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To work with datasets neeeded newer pandas versions\n",
    "# needed at least version  0.24.1 and above. \n",
    "\n",
    "pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plots\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going from dataset to pandas and then to numpy\n",
    "# Here is i 'm creating from pandas numpy array'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = diabetes_data.to_pandas_dataframe()\n",
    "df.count()\n",
    "df_np=df.to_numpy()\n",
    "print (np.count_nonzero(df_np [:,0]))\n",
    "pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dir\n",
    "data_dir='./helen/data'\n",
    "\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dir \n",
    "os.listdir(data_dir)\n",
    "os.makedirs ('./helen/output',exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./helen/script/helen_train_simple2.py\n",
    "\n",
    "# the version 2 training - simple read and train\n",
    "\n",
    "from azureml.core import Dataset, Run\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "output_dir='./helen/output'\n",
    "os.makedirs ('./helen/output',exist_ok=True)\n",
    "\n",
    "run = Run.get_context()\n",
    "# get the input dataset by name\n",
    "dataset = run.input_datasets['diabetes_data']\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "df = dataset.to_pandas_dataframe()\n",
    "x_array=df.to_numpy()\n",
    "\n",
    "print ('helen is printing X dataframe cnt',df.count())\n",
    "print ('helen is printing X numpy cnt',np.count_nonzero(x_array [:,0]))\n",
    "\n",
    "dataset = run.input_datasets['diabetes_labels']\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "df = dataset.to_pandas_dataframe()\n",
    "y_array=df.to_numpy()\n",
    "\n",
    "\n",
    "print ('helen is printing y dataframe cnt',df.count())\n",
    "print ('helen is printing y numpy cnt',np.count_nonzero(y_array [:,0]))\n",
    "\n",
    "\n",
    "run.log('data cnt',df.count())\n",
    "\n",
    "\n",
    "# load diabetes dataset, a well-known small dataset that comes with scikit-learn\n",
    "# REading dataset from file \n",
    "# Below writing to file\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#X, y = load_diabetes(return_X_y = True)\n",
    "y=y_array\n",
    "X=x_array\n",
    "columns = ['age', 'gender', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "data = {\n",
    "   \"train\":{\"X\": X_train, \"y\": y_train},        \n",
    "   \"test\":{\"X\": X_test, \"y\": y_test}\n",
    "}\n",
    "reg = Ridge(alpha = 0.03)\n",
    "reg.fit(data['train']['X'], data['train']['y'])\n",
    "preds = reg.predict(data['test']['X'])\n",
    "print('Mean Squared Error is', mean_squared_error(preds, data['test']['y']))\n",
    "\n",
    " # Output the Mean Squared Error to the notebook and to the run\n",
    "run.log('mse', mean_squared_error(preds, data['test']['y']))\n",
    "\n",
    " # Save the model to the outputs directory for capture\n",
    "model_file = 'diabetes_helen.pkl'\n",
    "model_file_name=os.path.join(output_dir, model_file)\n",
    "\n",
    "joblib.dump(value = reg, filename = model_file_name);\n",
    "\n",
    "\n",
    "print(run.get_file_names())\n",
    "\n",
    "# upload the model file explicitly into artifacts \n",
    "run.upload_file(name = model_file_name, path_or_stream = model_file_name)\n",
    "\n",
    "# register model\n",
    "model = run.register_model(model_name='helen_test',model_path=model_file_name)\n",
    "print(model.name, model.id, model.version, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "for a in range (len(preds)):\n",
    "    print (str (preds[a]) + '  actual:' + str (y_test[a]) + ' actual ',  X_test[a] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working locally, Nov 2020 \n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Dataset, Model , Workspace, Experiment\n",
    "import os\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# read current workspace from file\n",
    "ws=Workspace.from_config()\n",
    "\n",
    "#Accessing dataset which is already registered \n",
    "diabetes_data = Dataset.get_by_name(workspace=ws, name='diabetes_data')\n",
    "diabetes_labels = Dataset.get_by_name(workspace=ws, name='diabetes_labels')\n",
    "\n",
    "\n",
    "\n",
    "data_dir='./helen/data'\n",
    "script_dir='./helen/script'\n",
    "experiment = Experiment(workspace=ws, name=\"local_python_run\")\n",
    "\n",
    "\n",
    "est = SKLearn(source_directory=script_dir,\n",
    "                entry_script='helen_train_simple2.py',\n",
    "                # pass dataset object as an input with name 'titanic'\n",
    "                inputs=[diabetes_data.as_named_input('diabetes_data'),diabetes_labels.as_named_input('diabetes_labels')],\n",
    "                #CORRECT\n",
    "                pip_packages = ['azureml-sdk','azureml-dataprep[fuse,pandas]'],\n",
    "                #conda_packages=['azureml-sdk','numpy','scikit-learn'],\n",
    "                #WORKS correctly  \n",
    "                compute_target='local'\n",
    "                #Wroks correctly \n",
    "                # compute_target=compute_target\n",
    "               )\n",
    "\n",
    "# Submit the estimator as part of your experiment run\n",
    "\n",
    "experiment_run = experiment.submit(est)\n",
    "\n",
    "RunDetails(experiment_run).show()\n",
    "\n",
    "#experiment_run = experiment.submit(est)\n",
    "experiment_run.wait_for_completion(show_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./helen/script/helen_train_simple.py\n",
    "\n",
    "# the version 1 training - simple read from disk of one file\n",
    "\n",
    "from azureml.core import Dataset, Run\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "output_dir='./helen/output'\n",
    "\n",
    "run = Run.get_context()\n",
    "# get the input dataset by name\n",
    "dataset = run.input_datasets['diabetes_data']\n",
    "\n",
    "os.makedirs ('./helen/output',exist_ok=True)\n",
    "\n",
    "    \n",
    "# load the TabularDataset to pandas DataFrame\n",
    "df = dataset.to_pandas_dataframe()\n",
    "df_np=df.to_numpy()\n",
    "\n",
    "\n",
    "print ('helen is printing dataframe cnt',df.count())\n",
    "print ('helen is printing numpy cnt',np.count_nonzero(df_np [:,0]))\n",
    "\n",
    "run.log('data cnt',df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models\n",
    "from azureml.core.model import Model\n",
    "Model.list(ws )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all models called \"best_model\" and display their version numbers\n",
    "from azureml.core.model import Model\n",
    "models = Model.list(ws, name='helen_test')\n",
    "for m in models:\n",
    "    print(m.name, m.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCORING\n",
    "# Downlaoding it, in order to use it for scoring\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "import os\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "\n",
    "modelname='helen_test'\n",
    "model_file= \"diabetes_helen.pkl\"\n",
    "\n",
    "output_dir='./helen/download'\n",
    "os.makedirs (output_dir,exist_ok=True)\n",
    "\n",
    "model = Model(ws, modelname, version=4)\n",
    "model.download(target_dir=output_dir, exist_ok=True)\n",
    "print (model)\n",
    "\n",
    "# verify the downloaded model file\n",
    "model_file_name = os.path.join(output_dir, model_file)\n",
    "os.stat(model_file_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCORING\n",
    "\n",
    "# Scoring with model, which was downloaded earlier\n",
    "#load the model from disk and predict\n",
    "from sklearn.externals import joblib\n",
    "# REading dataset from web\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "loaded_model = joblib.load(open(model_file_name, 'rb'))\n",
    "print (model_file_name)\n",
    "\n",
    "#loading data\n",
    "X, y = load_diabetes(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "data = {\n",
    "   \"train\":{\"X\": X_train, \"y\": y_train},        \n",
    "   \"test\":{\"X\": X_test, \"y\": y_test}\n",
    "}\n",
    "# finding score\n",
    "result = loaded_model.score(X, y)\n",
    "print(result)\n",
    "\n",
    "# predicting values\n",
    "estimate = loaded_model.predict(X_test)\n",
    "\n",
    "for a in range (len(estimate)):\n",
    "    print (str (estimate[a]) + '  actual:' + str (y_test[a]) + ' input ', X_test [a])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run information\n",
    "minimum_rmse_runid = None\n",
    "minimum_rmse = None\n",
    "\n",
    "for run in experiment.get_runs():\n",
    "    if run.id=='test-experiment_1582140794_f8006a7f':\n",
    "        print (run.get_metrics())\n",
    "        print (run.get_details())\n",
    "    # each logged metric becomes a key in this returned dict\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run information\n",
    "# Finding the best run out of all runs\n",
    "minimum_rmse_runid = None\n",
    "minimum_rmse = None\n",
    "\n",
    "for run in experiment.get_runs():\n",
    "    # I use my run.id what i know\n",
    "    if run.id=='test-experiment_1582140794_f8006a7f':\n",
    "        run_metrics = run.get_metrics()\n",
    "        run_details = run.get_details()\n",
    "        # each logged metric becomes a key in this returned dict\n",
    "        run_rmse = run_metrics[\"mse\"]\n",
    "        run_id = run_details[\"runId\"]\n",
    "\n",
    "        if minimum_rmse is None:\n",
    "            minimum_rmse = run_rmse\n",
    "            minimum_rmse_runid = run_id\n",
    "        else:\n",
    "            if run_rmse < minimum_rmse:\n",
    "                minimum_rmse = run_rmse\n",
    "                minimum_rmse_runid = run_id\n",
    "\n",
    "print(\"Best run_id: \" + minimum_rmse_runid)\n",
    "print(\"Best run_id rmse: \" + str(minimum_rmse))\n",
    "\n",
    "# Best runs files\n",
    "from azureml.core import Run\n",
    "best_run = Run(experiment=experiment, run_id=minimum_rmse_runid)\n",
    "print(best_run.get_file_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_path = Model.get_model_path(model_name='helen_test')\n",
    "#print (model_path)\n",
    "# deserialize the model file back into a sklearn model\n",
    "\n",
    "#model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL SCRIPT\n",
    "# load diabetes dataset, a well-known small dataset that comes with scikit-learn\n",
    "# REading dataset from file \n",
    "# Below writing to file\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "X, y = load_diabetes(return_X_y = True)\n",
    "y=y_matrix\n",
    "X=X_matrix\n",
    "columns = ['age', 'gender', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "data = {\n",
    "   \"train\":{\"X\": X_train, \"y\": y_train},        \n",
    "   \"test\":{\"X\": X_test, \"y\": y_test}\n",
    "}\n",
    "reg = Ridge(alpha = 0.03)\n",
    "reg.fit(data['train']['X'], data['train']['y'])\n",
    "preds = reg.predict(data['test']['X'])\n",
    "print('Mean Squared Error is', mean_squared_error(preds, data['test']['y']))\n",
    "joblib.dump(value = reg, filename = 'model.pkl');\n",
    "\n",
    "for a in range (len(preds)):\n",
    "    print (str (preds[a]) + '  actual:' + str (y_test[a]) + ' actual ',  X_test[a] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#to list cureated environemnets\n",
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "for env in envs:\n",
    "    if env.startswith(\"AzureML\"):\n",
    "        print(\"Name\",env)\n",
    "        print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing environemnt out of box.\n",
    "\n",
    "from azureml.core import Environment\n",
    "\n",
    "curated_env = Environment.get(workspace=ws, name=\"AzureML-Minimal\")\n",
    "curated_env = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# To activate this environment, use:\n",
    "# > source activate /azureml-envs/azureml_b2ad30260b00c8bf1a18b629f070b89f\n",
    "#\n",
    "# To deactivate an active environment, use:\n",
    "# > source deactivate\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT YET TESTED; BUT SHOULD WORK \n",
    "\n",
    "from azureml.train.estimator import Estimator\n",
    "data_dir='./helen/data'\n",
    "script_dir='./helen/script'\n",
    "\n",
    "\n",
    "\n",
    "est = Estimator(source_directory=script_dir,\n",
    "                entry_script='helen_train.py',\n",
    "                # pass dataset object as an input with name 'titanic'\n",
    "                inputs=[diabetes_data.as_named_input('diabetes_data')],\n",
    "                #conda_packages=['azureml-sdk','numpy','scikit-learn'],\n",
    "                compute_target=compute_target,\n",
    "                environment_definition= curated_env\n",
    "               )\n",
    "\n",
    "# Submit the estimator as part of your experiment run\n",
    "experiment_run = experiment.submit(est)\n",
    "experiment_run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
