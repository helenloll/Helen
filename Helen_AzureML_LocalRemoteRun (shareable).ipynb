{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support notebook - Training locally and remotly\n",
    "\n",
    "\n",
    "### Version - Running code in docker image locally/remotely\n",
    "- docker image maintained by Azure ML for me, based on python version and libraries. Me, as user, do not maintain it\n",
    "-- locally in my computer \n",
    "-- remotely in remote compute\n",
    "\n",
    "### Version - Running code in specific docker image locally/remotely\n",
    "- specific docker image . In this example the specific docker image from Azure ML image registry, which was created by Azure ML service for me.\n",
    "-- locally in my computer \n",
    "-- remotely in remote compute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Datastore, ComputeTarget, RunConfiguration, Experiment\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.core import Environment\n",
    "from sklearn.impute import SimpleImputer\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read current workspace from file\n",
    "ws=Workspace.from_config()\n",
    "print (ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach compute \n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"automl-compute\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes,\n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(\n",
    "        ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing dataset which is already registered\n",
    "# get dataset by dataset name\n",
    "diabetes_data = Dataset.get_by_name(workspace=ws, name='diabetes_data')\n",
    "diabetes_labels = Dataset.get_by_name(workspace=ws, name='diabetes_labels')\n",
    "\n",
    "df = diabetes_data.to_pandas_dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./helen/script/helen_train_simple3.py\n",
    "\n",
    "# my training script - simple read and train\n",
    "\n",
    "from azureml.core import Dataset, Run\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "output_dir='./helen/output'\n",
    "os.makedirs ('./helen/output',exist_ok=True)\n",
    "\n",
    "run = Run.get_context()\n",
    "# get the input dataset by name\n",
    "dataset = run.input_datasets['diabetes_data']\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "df = dataset.to_pandas_dataframe()\n",
    "x_array=df.to_numpy()\n",
    "\n",
    "print ('helen is printing X dataframe cnt',df.count())\n",
    "print ('helen is printing X numpy cnt',np.count_nonzero(x_array [:,0]))\n",
    "\n",
    "dataset = run.input_datasets['diabetes_labels']\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "df = dataset.to_pandas_dataframe()\n",
    "y_array=df.to_numpy()\n",
    "\n",
    "\n",
    "print ('helen is printing y dataframe cnt',df.count())\n",
    "print ('helen is printing y numpy cnt',np.count_nonzero(y_array [:,0]))\n",
    "\n",
    "\n",
    "run.log('data cnt',df.count())\n",
    "\n",
    "\n",
    "# load diabetes dataset, a well-known small dataset that comes with scikit-learn\n",
    "# REading dataset from file \n",
    "# Below writing to file\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#X, y = load_diabetes(return_X_y = True)\n",
    "y=y_array\n",
    "X=x_array\n",
    "columns = ['age', 'gender', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "data = {\n",
    "   \"train\":{\"X\": X_train, \"y\": y_train},        \n",
    "   \"test\":{\"X\": X_test, \"y\": y_test}\n",
    "}\n",
    "reg = Ridge(alpha = 0.03)\n",
    "reg.fit(data['train']['X'], data['train']['y'])\n",
    "preds = reg.predict(data['test']['X'])\n",
    "print('Mean Squared Error is', mean_squared_error(preds, data['test']['y']))\n",
    "\n",
    " # Output the Mean Squared Error to the notebook and to the run\n",
    "run.log('mse', mean_squared_error(preds, data['test']['y']))\n",
    "\n",
    " # Save the model to the outputs directory for capture\n",
    "model_file = 'diabetes_helen.pkl'\n",
    "model_file_name=os.path.join(output_dir, model_file)\n",
    "\n",
    "joblib.dump(value = reg, filename = model_file_name);\n",
    "\n",
    "\n",
    "print(run.get_file_names())\n",
    "\n",
    "# upload the model file explicitly into artifacts \n",
    "run.upload_file(name = model_file_name, path_or_stream = model_file_name)\n",
    "\n",
    "# register model\n",
    "model = run.register_model(model_name='helen_test',model_path=model_file_name)\n",
    "print(model.name, model.id, model.version, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "for a in range (len(preds)):\n",
    "    print (str (preds[a]) + '  actual:' + str (y_test[a]) + ' actual ',  X_test[a] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - local and remote\n",
    "\n",
    "### Version - Running code in docker image locally/remotely\n",
    "- docker image maintained by Azure ML for me, based on python version and libraries. Me, as user, do not maintain it\n",
    "-- locally in my computer \n",
    "-- remotely in remote compute\n",
    "\n",
    "### Version - Running code in specific docker image locally/remotely\n",
    "- specific docker image . In this example the specific docker image from Azure ML image registry, which was created by Azure ML service for me.\n",
    "-- locally in my computer \n",
    "-- remotely in remote compute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version - Running code in docker image locally/remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working locally, Nov 2020 \n",
    "# Azure ML will create for me docker image \n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "data_dir='./helen/data'\n",
    "script_dir='./helen/script'\n",
    "experiment = Experiment(workspace=ws, name=\"local_python_run\")\n",
    "\n",
    "\n",
    "est = SKLearn(source_directory=script_dir,\n",
    "                entry_script='helen_train_simple3.py',\n",
    "                # pass dataset object as an input with name 'titanic'\n",
    "                inputs=[diabetes_data.as_named_input('diabetes_data'),diabetes_labels.as_named_input('diabetes_labels')],\n",
    "                #CORRECT\n",
    "                pip_packages = ['azureml-sdk','azureml-dataprep[fuse,pandas]'],\n",
    "                #conda_packages=['azureml-sdk','numpy','scikit-learn'],\n",
    "                #WORKS correctly  \n",
    "                compute_target='local'\n",
    "                #Wroks correctly \n",
    "                # compute_target=compute_target\n",
    "               )\n",
    "\n",
    "# Submit the estimator as part of your experiment run\n",
    "\n",
    "experiment_run = experiment.submit(est)\n",
    "\n",
    "RunDetails(experiment_run).show()\n",
    "\n",
    "#experiment_run = experiment.submit(est)\n",
    "experiment_run.wait_for_completion(show_output=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working remotely Nov 2020\n",
    "# Azure ML will create for me docker image \n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "data_dir='./helen/data'\n",
    "script_dir='./helen/script'\n",
    "\n",
    "\n",
    "\n",
    "est = SKLearn(source_directory=script_dir,\n",
    "                entry_script='helen_train_simple3.py',\n",
    "                # pass dataset object as an input \n",
    "                inputs=[diabetes_data.as_named_input('diabetes_data'),diabetes_labels.as_named_input('diabetes_labels')],\n",
    "                #CORRECT\n",
    "                pip_packages = ['azureml-sdk','azureml-dataprep[fuse,pandas]'],\n",
    "                #conda_packages=['azureml-sdk','numpy','scikit-learn'],\n",
    "                #WORKS correctly  \n",
    "                #compute_target='local'\n",
    "                #Wroks correctly \n",
    "                compute_target=compute_target\n",
    "               )\n",
    "\n",
    "# Submit the estimator as part of your experiment run\n",
    "\n",
    "experiment_run = experiment.submit(est)\n",
    "\n",
    "RunDetails(experiment_run).show()\n",
    "\n",
    "#experiment_run = experiment.submit(est)\n",
    "experiment_run.wait_for_completion(show_output=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version - Running code in specific docker image locally/remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING MY OWN DOCKER IMAGE - Image registry maintained by system in Azure ML\n",
    "# this image was created my AzureML for me in previous run of Estimator step when providing libraries\n",
    "\n",
    "ws.get_details() \n",
    "print (ws.get_details())\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "# you can also point to an image in a private ACR\n",
    "\n",
    "# that image was create my AzureML for me in previo Estimator step when providing libraries\n",
    "custom_docker_image='azureml/azureml_eb5994d85083050810e56c9d1fa49cbd'\n",
    "\n",
    "# Here is my very own azure Ml container registry \n",
    "image_registry_details = ContainerRegistry()\n",
    "image_registry_details.address = 'helenmachine59dbf329.azurecr.io'\n",
    "image_registry_details.username='helenmachine59dbf329'\n",
    "image_registry_details.password='v'\n",
    "print (image_registry_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING MY DOCKER IMAGE Nov 2020\n",
    "# BOTH LOCALLY or REMOTELY . here remotely\n",
    "# this image was created my AzureML for me in previous run of Estimator step when providing libraries\n",
    "\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.train.sklearn import SKLearn\n",
    "data_dir='./helen/data'\n",
    "script_dir='./helen/script'\n",
    "experiment = Experiment(workspace=ws, name=\"remote_docker_run\")\n",
    "\n",
    "# use a custom Docker image\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "\n",
    "\n",
    "\n",
    "# don't let the system build a new conda environment\n",
    "user_managed_dependencies = True\n",
    "\n",
    "est = SKLearn(source_directory=script_dir,\n",
    "                entry_script='helen_train_simple3.py',\n",
    "                # pass dataset object as an input with name \n",
    "                inputs=[diabetes_data.as_named_input('diabetes_data'),diabetes_labels.as_named_input('diabetes_labels')],\n",
    "                #CORRECT\n",
    "                #pip_packages = ['azureml-sdk','azureml-dataprep[fuse,pandas]'],\n",
    "                #conda_packages=['azureml-sdk','numpy','scikit-learn'],\n",
    "                #WORKS correctly  \n",
    "                #compute_target='local',\n",
    "                #Wroks correctly \n",
    "                compute_target=compute_target,\n",
    "                custom_docker_image='azureml/azureml_eb5994d85083050810e56c9d1fa49cbd',\n",
    "                user_managed=user_managed_dependencies,\n",
    "                image_registry_details=image_registry_details\n",
    "               )\n",
    "\n",
    "\n",
    "# Submit the estimator as part of your experiment run II way \n",
    "experiment_run = experiment.submit(est)\n",
    "RunDetails(experiment_run).show()\n",
    "experiment_run.wait_for_completion(show_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# USING MY DOCKER IMAGE  Nov 2020\n",
    "# BOTH LOCALLY or REMOTELY . here locally\n",
    "# this image was created my AzureML for me in previous run of Estimator step when providing libraries\n",
    "\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.widgets import RunDetails\n",
    "data_dir='./helen/data'\n",
    "script_dir='./helen/script'\n",
    "experiment = Experiment(workspace=ws, name=\"local_docker_run\")\n",
    "\n",
    "# use a custom Docker image\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# don't let the system build a new conda environment\n",
    "user_managed_dependencies = True\n",
    "\n",
    "est = SKLearn(source_directory=script_dir,\n",
    "                entry_script='helen_train_simple3.py',\n",
    "                # pass dataset object as an input \n",
    "                inputs=[diabetes_data.as_named_input('diabetes_data'),diabetes_labels.as_named_input('diabetes_labels')],\n",
    "                #CORRECT\n",
    "                #pip_packages = ['azureml-sdk','azureml-dataprep[fuse,pandas]'],\n",
    "                #conda_packages=['azureml-sdk','numpy','scikit-learn'],\n",
    "                #WORKS correctly  \n",
    "                compute_target='local',\n",
    "                #Wroks correctly \n",
    "                #compute_target=compute_target,\n",
    "                custom_docker_image='azureml/azureml_eb5994d85083050810e56c9d1fa49cbd',\n",
    "                user_managed=user_managed_dependencies,\n",
    "                image_registry_details=image_registry_details\n",
    "               )\n",
    "\n",
    "\n",
    "# Submit the estimator as part of your experiment run II way \n",
    "experiment_run = experiment.submit(est)\n",
    "RunDetails(experiment_run).show()\n",
    "experiment_run.wait_for_completion(show_output=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check run\n",
    "from azureml.widgets import RunDetails\n",
    "experiment_run = experiment.submit(est)\n",
    "\n",
    "RunDetails(experiment_run).show()\n",
    "experiment_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# To activate this environment, use:\n",
    "# > source activate /azureml-envs/azureml_b2ad30260b00c8bf1a18b629f070b89f\n",
    "#\n",
    "# To deactivate an active environment, use:\n",
    "# > source deactivate\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT YET TESTED; BUT SHOULD WORK \n",
    "\n",
    "from azureml.train.estimator import Estimator\n",
    "data_dir='./helen/data'\n",
    "script_dir='./helen/script'\n",
    "\n",
    "\n",
    "\n",
    "est = Estimator(source_directory=script_dir,\n",
    "                entry_script='helen_train.py',\n",
    "                # pass dataset object as an input with name 'titanic'\n",
    "                inputs=[diabetes_data.as_named_input('diabetes_data')],\n",
    "                #conda_packages=['azureml-sdk','numpy','scikit-learn'],\n",
    "                compute_target=compute_target,\n",
    "                environment_definition= curated_env\n",
    "               )\n",
    "\n",
    "# Submit the estimator as part of your experiment run\n",
    "experiment_run = experiment.submit(est)\n",
    "experiment_run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
