{
  "metadata": {
    "saveOutput": false,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure Machine Learning - End to End example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from azureml.core import Workspace, Dataset\n",
        "from azureml.core import Environment\n",
        "from azureml.core import Datastore, Dataset, Workspace, Experiment, RunConfiguration\n",
        "\n",
        "# Check versions\n",
        "import azureml.core\n",
        "import sklearn\n",
        "import joblib\n",
        "import pandas\n",
        "\n",
        "print(\"Azure SDK version:\", azureml.core.VERSION)\n",
        "print('scikit-learn version is {}.'.format(sklearn.__version__))\n",
        "print('joblib version is {}.'.format(joblib.__version__))\n",
        "print('pandas version is {}.'.format(pandas.__version__))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conencting to Azure ML \n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "subscription_id = \"8b2f4e94-e7b0-42e5-b775-dd2d5968c4e6\"\n",
        "resource_group = \"HelenMachineLearning\"\n",
        "workspace_name = \"HelenMachineLearning\"\n",
        "workspace_name = \"HelenDatabricksLearning\"\n",
        "\n",
        "\n",
        "ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
        "\n",
        "\n",
        "#ws = Workspace.from_config()\n",
        "print( ws.name, ws.resource_group, ws.location, ws.subscription_id,  sep = '\\n')\n",
        "print('my workspace: '+ ws.name )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working wiht dat ain Azure ML workspace\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "#CREATING DATASTORE in Azure ML Workspace. \n",
        "# You create it once\n",
        "\n",
        "from azureml.core import Workspace, Experiment, Datastore, Dataset\n",
        "\n",
        "blob_datastore_name='helen_blobstore' # Name of the datastore to workspace\n",
        "container_name=os.getenv(\"BLOB_CONTAINER\", \"helenml\") # Name of Azure blob container\n",
        "account_name=os.getenv(\"BLOB_ACCOUNTNAME\", \"storagehelen\") # Storage account name\n",
        "account_key=os.getenv(\"BLOB_ACCOUNT_KEY\", \"g==\") # Storage account key\n",
        "\n",
        "helen_datastore = Datastore.register_azure_blob_container(workspace=ws, \n",
        "                                                         datastore_name=blob_datastore_name, \n",
        "                                                         container_name=container_name, \n",
        "                                                         account_name=account_name,\n",
        "                                                         account_key=account_key)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Fetching all datastores i have in Azure ML Workspace\n",
        "\n",
        "datastore = ws.get_default_datastore()\n",
        "print (\"my default datastore: \"+ datastore.name, sep = '\\n')\n",
        "\n",
        "datastore = Datastore.get_default (workspace=ws)\n",
        "print ('my default datastore: ' + datastore.name)\n",
        "     \n",
        "helen_datastore = Datastore.get(workspace=ws, datastore_name='helen_blobstore')\n",
        "print ('my helen_datastore datastore: ' + helen_datastore.name)\n",
        "\n",
        "# List all datastores registered in the current workspace\n",
        "datastores = ws.datastores\n",
        "print ('all attached datasores :')\n",
        "for name, datastore in datastores.items():\n",
        "    print ('datastore name :',  name, ' Def: ', datastore.datastore_type, datastore.account_name, datastore.container_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "outputCollapsed": true
      },
      "source": [
        "# Data upload\n",
        "# Uploading data files once, from PC works, not here in Synapse because of it requires access to VM disks\n",
        "helen_datastore = Datastore.get(workspace=ws, datastore_name='helen_blobstore')\n",
        "\n",
        "# Correct\n",
        "helen_datastore.upload_files(files = ['./helen/data/diabetes.csv'],\n",
        "                       target_path = '/helen/data',\n",
        "                       overwrite = False,\n",
        "                       show_progress = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Registering Tabular data ONCE in Azure ML Workspace\n",
        "\n",
        "datastore = ws.get_default_datastore()\n",
        "\n",
        "helen_datastore = Datastore.get(workspace=ws, datastore_name='helen_blobstore')\n",
        "\n",
        "##########################\n",
        "#diabetes data\n",
        "##############\n",
        "diabetes = Dataset.Tabular.from_delimited_files(path=[(helen_datastore, '/helen/data/diabetes.csv')],separator=',')\n",
        "diabetes = diabetes_data.register(workspace=ws, \n",
        "                                 name='diabetes',\n",
        "                                 description='diabetes data and labels',\n",
        "                                 create_new_version=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Accessing dataset which is already registered\n",
        "# get dataset by dataset name\n",
        "diabetes = Dataset.get_by_name(workspace=ws, name='diabetes')\n",
        "\n",
        "d_data = diabetes.to_pandas_dataframe()\n",
        "d_data.head(10)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remote compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Attache Azure ML Compute as Cluster of low cost nodes\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget\n",
        "import os\n",
        "\n",
        "# choose a name for your cluster\n",
        "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"automl-compute\")\n",
        "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
        "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
        "\n",
        "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
        "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
        "\n",
        "\n",
        "if compute_name in ws.compute_targets:\n",
        "    compute_target = ws.compute_targets[compute_name]\n",
        "    if compute_target and type(compute_target) is AmlCompute:\n",
        "        print('found compute target. just use it. ' + compute_name)\n",
        "else:\n",
        "    print('creating a new compute target...')\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
        "                                                                min_nodes=compute_min_nodes,\n",
        "                                                                max_nodes=compute_max_nodes)\n",
        "\n",
        "    # create the cluster\n",
        "    compute_target = ComputeTarget.create(\n",
        "        ws, compute_name, provisioning_config)\n",
        "\n",
        "    # can poll for a minimum number of nodes and for a specific timeout.\n",
        "    # if no min node count is provided it will use the scale settings for the cluster\n",
        "    compute_target.wait_for_completion(\n",
        "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "\n",
        "    # For a more detailed view of current AmlCompute status, use get_status()\n",
        "    print(compute_target.get_status().serialize())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### we write script OS disk here, as we do not have access to VM disks\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Here in Synapse commented out\n",
        "# create a directory in my local computer\n",
        "#script_dir = './helen_script'\n",
        "#os.makedirs(script_dir, exist_ok=True)\n",
        "#os.listdir(script_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "import os\n",
        "script_dir = 'helen_script'\n",
        "script_file='helen_script/helen.py'\n",
        "print ('before')\n",
        "print (os.listdir(script_dir))\n",
        "\n",
        "if path.isfile(script_file) is True:\n",
        "    os.remove (script_file)\n",
        "\n",
        "print ('after')\n",
        "print (os.listdir(script_dir))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "#%%writefile ./helen/script/diabetes2021_prep.py\n",
        "\n",
        "# Writing helen.py file to VM disk\n",
        "a_content=\"\"\"\n",
        "import parser\n",
        "from azureml.core import Dataset, Run\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.externals import joblib\n",
        "import joblib\n",
        "from matplotlib import pyplot as plot\n",
        "from azureml.core import Workspace, Datastore\n",
        "\n",
        "##########################################\n",
        "##########################################\n",
        "# AML content - start\n",
        "##########################################\n",
        "##########################################\n",
        "\n",
        "print ('HELEN PREP STEP ')\n",
        "output_dir='./helen/output'\n",
        "os.makedirs ('./helen/output',exist_ok=True)\n",
        "run = Run.get_context()\n",
        "ws = run.experiment.workspace\n",
        "\n",
        "##########################################\n",
        "# get arguments 2021\n",
        "##########################################\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--kernel', type=str, default='linear',\n",
        "                        help='Kernel type to be used in the algorithm')\n",
        "parser.add_argument('--ridge', type=float, default=0.03,\n",
        "                        help='Penalty parameter of the error term')\n",
        "parser.add_argument('--ds', type=str, dest='dataset_id')\n",
        "\n",
        "\n",
        "args = parser.parse_args()\n",
        "run.log('Kernel type', np.str(args.kernel))\n",
        "run.log('Ridge', np.float(args.ridge))\n",
        "\n",
        "\n",
        "##########################################\n",
        "# get the input dataset by name\n",
        "##########################################\n",
        "\n",
        "\n",
        "dataset = run.input_datasets['diabetes']\n",
        "# load the TabularDataset to pandas DataFrame\n",
        "df = dataset.to_pandas_dataframe()\n",
        "\n",
        "# THIS IS ALSO CORRECT \n",
        "dataset = Dataset.get_by_id(ws, id=args.dataset_id)\n",
        "df = dataset.to_pandas_dataframe()\n",
        "\n",
        "\n",
        "dd_data=df\n",
        "dd_data=dd_data.drop(columns=[\"Target\"])\n",
        "x_array=dd_data.to_numpy()\n",
        "print (\"correct x !!!! \", type (dd_data))\n",
        "run.log('data x cnt',df.count())\n",
        "\n",
        "dd_target=df\n",
        "dd_target=dd_target[[\"Target\"]]\n",
        "y_array=dd_target.to_numpy()\n",
        "print (\"correct y !!!! \", type (dd_data))\n",
        "run.log('data y cnt',df.count())\n",
        "\n",
        "##########################################\n",
        "##########################################\n",
        "\n",
        "\n",
        "run.log('data cnt',df.count())\n",
        "\n",
        "##########################################\n",
        "##########################################\n",
        "# AML content - end\n",
        "##########################################\n",
        "##########################################\n",
        "\n",
        "\n",
        "# My regural python code\n",
        "y=y_array\n",
        "X=x_array\n",
        "columns = ['age', 'gender', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "data = {\n",
        "   \"train\":{\"X\": X_train, \"y\": y_train},        \n",
        "   \"test\":{\"X\": X_test, \"y\": y_test}\n",
        "}\n",
        "reg = Ridge(alpha = 0.03)\n",
        "reg = Ridge(alpha = np.float(args.ridge))\n",
        "\n",
        "reg.fit(data['train']['X'], data['train']['y'])\n",
        "preds = reg.predict(data['test']['X'])\n",
        "print('Mean Squared Error is', mean_squared_error(preds, data['test']['y']))\n",
        "\n",
        "\n",
        "##########################################\n",
        "# Huper parameter tuning\n",
        "##########################################\n",
        "# model accuracy for X_test\n",
        "mse= mean_squared_error(preds, data['test']['y'])\n",
        "run.log('Accuracy', np.float(mse))\n",
        "\n",
        "##########################################\n",
        "##########################################\n",
        "# AML content - start\n",
        "##########################################\n",
        "##########################################\n",
        "\n",
        "# Log mse in Azure ML logs\n",
        "run.log('mse',  np.float(mse))\n",
        "\n",
        "# Save the model to the outputs directory for capture\n",
        "model_file = 'diabetes_helen.pkl'\n",
        "model_file_name=os.path.join(output_dir, model_file)\n",
        "joblib.dump(value = reg, filename = model_file_name);\n",
        "print(run.get_file_names())\n",
        "\n",
        "# upload the model file explicitly into artifacts Azure ML artifacts\n",
        "run.upload_file(name = model_file_name, path_or_stream = model_file_name)\n",
        "\n",
        "# register model in Azure ML Resitry \n",
        "model = run.register_model(model_name='helen_test',model_path=model_file_name)\n",
        "print(model.name, model.id, model.version, sep='\\t')\n",
        "\n",
        "for a in range (len(preds)):\n",
        "    run.log_row(\"Error: Estimate  - Actual\", x=a, y=abs (float (preds[a]) - float(y_test[a])))\n",
        "    \n",
        "\n",
        "# Creating file to oputput\n",
        "num_rows, num_cols = X_test.shape\n",
        "pred = preds.reshape((num_rows, 1))\n",
        "actual=y_test.reshape((num_rows, 1))\n",
        "\n",
        "tmp_npy = np.append (X_test, actual, 1)\n",
        "helen_numpy = np.append (tmp_npy, pred, 1)\n",
        "print ('helen_numpy shape ',helen_numpy.shape)\n",
        "\n",
        "helen_pandas=pd.DataFrame(data=helen_numpy)\n",
        "\n",
        "LOCALFILENAME='helen_score_file.txt'\n",
        "score_dir='./logs'\n",
        "score_dir='./helen/score'\n",
        "\n",
        "# Uploading file as articraft\n",
        "os.makedirs (score_dir,exist_ok=True)\n",
        "score_file = os.path.join(score_dir, LOCALFILENAME) \n",
        "helen_pandas.to_csv(score_file, sep=',', encoding='utf-8', index=False)\n",
        "print ('file name', score_file)\n",
        "\n",
        "# upload scored data explicitly into artifacts \n",
        "run.upload_file(name = score_file, path_or_stream = score_file)\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "# Plots\n",
        "##########################################\n",
        "\n",
        "    \n",
        "# Logging histogram plot in Azue ML \n",
        "num_rows, num_cols = X_test.shape\n",
        "pred = preds.reshape((num_rows, 1))\n",
        "actual=y_test.reshape((num_rows, 1))\n",
        "tmp_npy = np.append (X_test, actual, 1)\n",
        "helen_numpy = np.append (tmp_npy, pred, 1)\n",
        "\n",
        "\n",
        "f=helen_numpy\n",
        "print (f.shape)\n",
        "fnrow=f.shape[0]\n",
        "fncol=f.shape[1]\n",
        "print ( \" rows \", fnrow, \"columns \", fncol)\n",
        "\n",
        "# Histograms to all columns\n",
        "i=0\n",
        "for i in range (fncol):\n",
        "    title= str (i) + ' nr column  '\n",
        "    plot.title(title)\n",
        "    plot.hist (f[:,[i]],bins=30,color='blue',edgecolor='white')\n",
        "    #CORRECTplot.show()\n",
        "    run.log_image ('Helen plot_' + str (i),plot=plot)\n",
        "    plot.clf()\n",
        "\n",
        "##########################################\n",
        "##########################################\n",
        "# AML content - end\n",
        "##########################################\n",
        "##########################################\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "# create output refernce for dataset in pipeline step\n",
        "##########################################\n",
        "\n",
        "#mounted_output_path = os.environ['AZUREML_DATAREFERENCE_diabetes_temp_ds']\n",
        "#os.makedirs(mounted_output_path, exist_ok=True)\n",
        "#score_file = os.path.join(mounted_output_path, LOCALFILENAME) \n",
        "#helen_pandas.to_csv(score_file, sep=',', encoding='utf-8', index=False)\n",
        "#print ('file name to somewhere', score_file)\n",
        "\n",
        "\"\"\"\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "script_dir = 'helen_script'\n",
        "script_file='helen_script/helen.py' \n",
        "\n",
        "os.makedirs (script_dir,exist_ok=True)\n",
        "myfile=open(script_file,\"a\")\n",
        "#a_content=\"print ('hello helen');\"\n",
        "myfile.write(a_content)\n",
        "myfile.close"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "script_dir = 'helen_script'\n",
        "script_file='helen_script/helen.py' \n",
        "\n",
        "os.makedirs (script_dir,exist_ok=True)\n",
        "myfile=open(script_file,\"a\")\n",
        "a_dummy=\" \"\n",
        "myfile.write(a_dummy)\n",
        "myfile.close"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "import os.path\n",
        "from os import path\n",
        "\n",
        "os.makedirs (script_dir,exist_ok=True)\n",
        "print (path.exists(script_dir))\n",
        "\n",
        "#script_file='helen_script/helen'\n",
        "print (path.isfile(script_file))\n",
        "\n",
        "print ('current folder:', os.getcwd()  )\n",
        "#print ('current directory content ',os.listdir(script_dir) )\n",
        "print ('current directory content ',os.listdir('.') )\n",
        "\n",
        "# peek at contents\n",
        "with open(script_file) as file:\n",
        "   print(file.read())\n",
        "\n",
        "# Your current directory is like this\n",
        "#/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1620324866325_0003/container_1620324866325_0003_01_000001/helen_script/helen_script/helen"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparing for AML run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating environment - create it first by executing this notebook in Azure ML Workspace. Here, we use existing env\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%writefile sklearn_conda_dependencies.yml\n",
        "# Writing filw to OS disk , you should do abov, or use the envr created earlier\n",
        "\n",
        "dependencies:\n",
        "- python=3.6.2\n",
        "- scikit-learn\n",
        "- matplotlib\n",
        "- pip:\n",
        "  - azureml-defaults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Creating environemnt, right now we use existing. First execute this in ML workspace.# Create env from conda dependencies\n",
        "\n",
        "from azureml.core import Environment\n",
        "\n",
        "try: \n",
        "    sklearn_env =Environment.get(workspace=ws,name=\"sklearn-env\")\n",
        "    print ('environment exists ')\n",
        "    sklearn_env\n",
        "except: \n",
        "    sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = './sklearn_conda_dependencies.yml')\n",
        "    sklearn_env.docker.enabled = True\n",
        "    sklearn_env.python.user_managed_dependencies = False\n",
        "    sklearn_env.register(workspace = ws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "scrolled": true
      },
      "source": [
        "#create env from earlier saved environment\n",
        "sklearn_env =Environment.get(workspace=ws,name=\"sklearn-env\")\n",
        "#sklearn_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scriptrunconfig - is instruction for executing script in remote environemnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "# COMPLETE TEST\n",
        "###########################\n",
        "# THIS FULL AND COMPLETE\n",
        "###########################\n",
        "\n",
        "from azureml.core import ScriptRunConfig\n",
        "#from azureml.widgets import RunDetails\n",
        "\n",
        "\n",
        "# CORRECT script_file= 'diabetes2021_test.py'\n",
        "cript_dir = 'helen_script'\n",
        "script_file='helen.py' \n",
        "\n",
        "\n",
        "experiment = Experiment(workspace=ws, name=\"diabetes2021_env_tscp\")\n",
        "sklearn_env.docker.enabled = True\n",
        "\n",
        "est = ScriptRunConfig(source_directory=script_dir,\n",
        "                      script=script_file,\n",
        "                      arguments=['--kernel', 'linear', '--ridge', 0.03,'--ds', diabetes.as_named_input('diabetes')]\n",
        "                      #outputs=[diabetes_scored]\n",
        "                      )\n",
        "\n",
        "# Submit the estimator as part of your experiment run\n",
        "est.run_config.target=compute_target\n",
        "#est.run_config.target='local'\n",
        "\n",
        "est.run_config.environment=sklearn_env\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Single run "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "outputCollapsed": true
      },
      "source": [
        "# Single run\n",
        "experiment_run = experiment.submit(est)\n",
        "experiment_run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Not available\n",
        "#from azureml.widgets import RunDetails\n",
        "#RunDetails(experiment_run).show()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter tuning run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
        "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
        "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
        "from azureml.train.hyperdrive.parameter_expressions import choice\n",
        "    \n",
        "\n",
        "param_sampling = RandomParameterSampling( {\n",
        "    \"--kernel\": choice('linear', 'rbf'),\n",
        "    \"--ridge\": choice(0.01, 0.03, 0.05)\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "hyperdrive_config = HyperDriveConfig(run_config=est,\n",
        "                                     hyperparameter_sampling=param_sampling, \n",
        "                                     primary_metric_name='mse',\n",
        "                                     primary_metric_goal=PrimaryMetricGoal.MINIMIZE,\n",
        "                                     max_total_runs=12,\n",
        "                                     max_concurrent_runs=4)\n",
        "\n",
        "# start the HyperDrive run\n",
        "hyperdrive_run = experiment.submit(hyperdrive_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "scrolled": true
      },
      "source": [
        "# not avilable \n",
        "# RunDetails(hyperdrive_run).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "hyperdrive_run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# To get old hold of run\n",
        "from azureml.core import Workspace, Dataset, Run\n",
        "experiment = Experiment(workspace=ws, name=\"diabetes2021_env_tscp\")\n",
        "\n",
        "#To get hold run : hyperdrive_run=Run(experiment, run_id='HD_87f80642-010e-444d-bbc2-53d78bdd0379', outputs=None)\n",
        "\n",
        "#hyperdrive_run=Experiment.run_id\n",
        "print (type (hyperdrive_run)   )\n",
        "\n",
        "hyperdrive_run.get_details\n",
        "hyperdrive_run.get_metrics()\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "for a in hyperdrive_run.get_children():\n",
        "    print (a)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
        "print(best_run.get_details()['runDefinition']['arguments'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "a= hyperdrive_run.get_children_sorted_by_primary_metric()\n",
        "for aa in a:\n",
        "    print(aa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "for a in best_run.get_file_names():\n",
        "    print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "model = best_run.register_model(model_name='diabates_hyper', model_path='helen/output/diabetes_helen.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "best_run.get_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix - scoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetching existing model and scoring locally\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Fetching model from Azure ML ws\n",
        "\n",
        "import os\n",
        "modelname='diabates_hyper'\n",
        "model_file= \"diabetes_helen.pkl\"\n",
        "\n",
        "model = Model(ws, modelname)\n",
        "\n",
        "output_dir='helen'\n",
        "os.makedirs (output_dir,exist_ok=True)\n",
        "\n",
        "#model = Model(ws, modelname, version=4)\n",
        "model.download(target_dir=output_dir, exist_ok=True)\n",
        "print (model)\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Scoring - i have here issue with sklearn versions\n",
        "\n",
        "import joblib as joblib\n",
        "model_file_name = os.path.join(output_dir, model_file)\n",
        "# checking file exists\n",
        "os.stat(model_file_name)\n",
        "\n",
        "# ready for scoring\n",
        "my_model = joblib.load(model_file_name)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Accessing dataset which is already registered\n",
        "# get dataset by dataset name\n",
        "diabetes = Dataset.get_by_name(workspace=ws, name='diabetes')\n",
        "\n",
        "d_data = diabetes.to_pandas_dataframe()\n",
        "d_data.head(10)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {},
      "source": [
        "import parser\n",
        "from azureml.core import Dataset, Run\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import joblib\n",
        "from matplotlib import pyplot as plot\n",
        "from azureml.core import Workspace, Datastore\n",
        "\n",
        "# load the TabularDataset or any dataset to pandas DataFrame \n",
        "df = diabetes.to_pandas_dataframe()\n",
        "\n",
        "\n",
        "dd_data=df\n",
        "dd_data=dd_data.drop(columns=[\"Target\"])\n",
        "x_array=dd_data.to_numpy()\n",
        "print (\"correct x !!!! \", type (dd_data))\n",
        "\n",
        "\n",
        "dd_target=df\n",
        "dd_target=dd_target[[\"Target\"]]\n",
        "y_array=dd_target.to_numpy()\n",
        "print (\"correct y !!!! \", type (dd_data))\n",
        "\n",
        "# My regural python code\n",
        "y=y_array\n",
        "X=x_array\n",
        "columns = ['age', 'gender', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
        "\n",
        "\n",
        "data = {\n",
        "   \"test\":{\"X\": X, \"y\": y}\n",
        "}\n",
        "preds = my_model.predict(data['test']['X'])\n",
        "mse= mean_squared_error(preds, data['test']['y']) \n",
        "print ('mse = ', mse)\n",
        "\n",
        "###########################\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = preds\n",
        "y = y\n",
        "#plt.plot(x,y)\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel(\"predicted\")\n",
        "plt.ylabel(\"actual\")\n",
        "plt.title('Predicted vs Actual')\n",
        "plt.show()\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enviroments\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Helper for environemnt\n",
        "from azureml.core import Environment\n",
        "\n",
        "envs = Environment.list(workspace=ws)\n",
        "\n",
        "# List Environments and packages in my workspace\n",
        "for env in envs:\n",
        "    if env.startswith(\"\"):\n",
        "    #if env.startswith(\"sk\"):\n",
        "        print(\"Name\",env)\n",
        "        print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n",
        "        \n",
        "# Use curated environment from AML named \"AzureML-Tutorial\"\n",
        "\n",
        "# Correct curated_environment = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\n",
        "# Correct Custom environment: Environment.get(workspace=ws,name=\"myenv\",version=\"1\")"
      ]
    }
  ]
}