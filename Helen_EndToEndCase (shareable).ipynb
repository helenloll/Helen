{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning - End to End example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.core import Environment\n",
    "from azureml.core import Datastore, Dataset, Workspace, Experiment, RunConfiguration\n",
    "\n",
    "# Check versions\n",
    "import azureml.core\n",
    "import sklearn\n",
    "import joblib\n",
    "import pandas\n",
    "\n",
    "print(\"Azure SDK version:\", azureml.core.VERSION)\n",
    "print('scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "print('joblib version is {}.'.format(joblib.__version__))\n",
    "print('pandas version is {}.'.format(pandas.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Accessing workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write workspace to file\n",
    "from azureml.core import Workspace\n",
    "\n",
    "subscription_id = '8b2f4e94-e7b0-42e5-b775-dd2d5968c4e6'\n",
    "resource_group  = 'HelenMachineLearning'\n",
    "workspace_name  = 'HelenMachineLearning'\n",
    "workspace_name  = 'HelenDatabricksLearning'\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    #CORRECT ws.write_config()\n",
    "    print( ws.name, ws.resource_group, ws.location, ws.subscription_id,  sep = '\\n')\n",
    "    print('my workspace: '+ ws.name )\n",
    "except:\n",
    "    ws = Workspace.from_config()\n",
    "    print( ws.name, ws.resource_group, ws.location, ws.subscription_id,  sep = '\\n')\n",
    "    print('my workspace: '+ ws.name )\n",
    "    print('Workspace found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing data in Azure ML workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING DATASTORE\n",
    "\n",
    "from azureml.core import Workspace, Experiment, Datastore, Dataset\n",
    "\n",
    "blob_datastore_name='helen_blobstore' # Name of the datastore to workspace\n",
    "container_name=os.getenv(\"BLOB_CONTAINER\", \"helenml\") # Name of Azure blob container\n",
    "account_name=os.getenv(\"BLOB_ACCOUNTNAME\", \"storagehelen\") # Storage account name\n",
    "account_key=os.getenv(\"BLOB_ACCOUNT_KEY\", \"sg==\") # Storage account key\n",
    "\n",
    "helen_datastore = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                         datastore_name=blob_datastore_name, \n",
    "                                                         container_name=container_name, \n",
    "                                                         account_name=account_name,\n",
    "                                                         account_key=account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading data files once\n",
    "helen_datastore = Datastore.get(workspace=ws, datastore_name='helen_blobstore')\n",
    "\n",
    "\n",
    "helen_datastore.upload_files(files = ['./helen/data/diabetes.csv'],\n",
    "                       target_path = '/helen/data',\n",
    "                       overwrite = False,\n",
    "                       show_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering Tabular data ONCE\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "helen_datastore = Datastore.get(workspace=ws, datastore_name='helen_blobstore')\n",
    "\n",
    "##########################\n",
    "#diabetes data\n",
    "##############\n",
    "diabetes = Dataset.Tabular.from_delimited_files(path=[(helen_datastore, '/helen/data/diabetes.csv')],separator=',')\n",
    "diabetes = diabetes.register(workspace=ws, \n",
    "                                 name='diabetes',\n",
    "                                 description='diabetes data and labels',\n",
    "                                 create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "print (\"my default datastore: \"+ datastore.name, sep = '\\n')\n",
    "\n",
    "datastore = Datastore.get_default (workspace=ws)\n",
    "print ('my default datastore: ' + datastore.name)\n",
    "     \n",
    "helen_datastore = Datastore.get(workspace=ws, datastore_name='helen_blobstore')\n",
    "print ('my helen_datastore datastore: ' + helen_datastore.name)\n",
    "\n",
    "# List all datastores registered in the current workspace\n",
    "datastores = ws.datastores\n",
    "print ('all attached datasores :')\n",
    "for name, datastore in datastores.items():\n",
    "    print ('datastore name :',  name, ' Def: ', datastore.datastore_type, datastore.account_name, datastore.container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing dataset which is already registered\n",
    "# get dataset by dataset name\n",
    "diabetes = Dataset.get_by_name(workspace=ws, name='diabetes')\n",
    "\n",
    "d_data = diabetes.to_pandas_dataframe()\n",
    "d_data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting working\n",
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My environment\n",
    "\n",
    "# CORRECT ws = Workspace.from_config()\n",
    "print( ws.name, ws.resource_group, ws.location, ws.subscription_id,  sep = '\\n')\n",
    "print('my workspace: '+ ws.name )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attache Azure ML Compute as Cluster of low cost nodes\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"automl-compute\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes,\n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(\n",
    "        ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory in my local comuter\n",
    "script_dir = './helen/script'\n",
    "os.makedirs(script_dir, exist_ok=True)\n",
    "os.listdir(script_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./helen/script/diabetes2021_test.py\n",
    "\n",
    "import argparse\n",
    "from azureml.core import Run, Dataset\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import joblib \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "run = Run.get_context()\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--kernel', type=str, default='linear',\n",
    "                        help='Kernel type to be used in the algorithm')\n",
    "parser.add_argument('--penalty', type=float, default=1.0,\n",
    "                        help='Penalty parameter of the error term')\n",
    "parser.add_argument('--ds', type=str, dest='dataset_id')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "run.log('Kernel type', np.str(args.kernel))\n",
    "run.log('Penalty', np.float(args.penalty))\n",
    "\n",
    "dataset = Dataset.get_by_id(ws, id=args.dataset_id)\n",
    "data = dataset.to_pandas_dataframe()\n",
    "print ('by id')\n",
    "print (data.head(10))\n",
    "\n",
    "#INCORRECT SYNTAX dataset_ = run.input_datasets[args.dataset_id]\n",
    "dataset = run.input_datasets['diabetes']\n",
    "data = dataset.to_pandas_dataframe()\n",
    "print ('by name')\n",
    "print (data.head(10))\n",
    "\n",
    "\n",
    "###########################\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(0, 3.47*2, 0.05)\n",
    "y = np.tan(x)\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"angle\")\n",
    "plt.ylabel(\"Tan value\")\n",
    "plt.title('Tan wave')\n",
    "plt.show()\n",
    "\n",
    "############################\n",
    "\n",
    "run.log('Accuracy', np.average(y))\n",
    "\n",
    "\n",
    "run.log_image ('Helen plot',plot=plt)\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./helen/script/diabetes2021_prep.py\n",
    "\n",
    "# Full example\n",
    "\n",
    "import parser\n",
    "from azureml.core import Dataset, Run\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.externals import joblib\n",
    "import joblib\n",
    "from matplotlib import pyplot as plot\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "# AML content - start\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "print ('HELEN PREP STEP ')\n",
    "output_dir='./helen/output'\n",
    "os.makedirs ('./helen/output',exist_ok=True)\n",
    "run = Run.get_context()\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "##########################################\n",
    "# get arguments 2021\n",
    "##########################################\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--kernel', type=str, default='linear',\n",
    "                        help='Kernel type to be used in the algorithm')\n",
    "parser.add_argument('--ridge', type=float, default=0.03,\n",
    "                        help='Penalty parameter of the error term')\n",
    "parser.add_argument('--ds', type=str, dest='dataset_id')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "run.log('Kernel type', np.str(args.kernel))\n",
    "run.log('Ridge', np.float(args.ridge))\n",
    "\n",
    "\n",
    "##########################################\n",
    "# get the input dataset by name\n",
    "##########################################\n",
    "\n",
    "\n",
    "dataset = run.input_datasets['diabetes']\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "df = dataset.to_pandas_dataframe()\n",
    "\n",
    "# THIS IS ALSO CORRECT \n",
    "dataset = Dataset.get_by_id(ws, id=args.dataset_id)\n",
    "df = dataset.to_pandas_dataframe()\n",
    "\n",
    "\n",
    "dd_data=df\n",
    "dd_data=dd_data.drop(columns=[\"Target\"])\n",
    "x_array=dd_data.to_numpy()\n",
    "print (\"correct x !!!! \", type (dd_data))\n",
    "run.log('data x cnt',df.count())\n",
    "\n",
    "dd_target=df\n",
    "dd_target=dd_target[[\"Target\"]]\n",
    "y_array=dd_target.to_numpy()\n",
    "print (\"correct y !!!! \", type (dd_data))\n",
    "run.log('data y cnt',df.count())\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "\n",
    "run.log('data cnt',df.count())\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "# AML content - end\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "\n",
    "# My regural python code\n",
    "y=y_array\n",
    "X=x_array\n",
    "columns = ['age', 'gender', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "data = {\n",
    "   \"train\":{\"X\": X_train, \"y\": y_train},        \n",
    "   \"test\":{\"X\": X_test, \"y\": y_test}\n",
    "}\n",
    "reg = Ridge(alpha = 0.03)\n",
    "reg = Ridge(alpha = np.float(args.ridge))\n",
    "\n",
    "reg.fit(data['train']['X'], data['train']['y'])\n",
    "preds = reg.predict(data['test']['X'])\n",
    "print('Mean Squared Error is', mean_squared_error(preds, data['test']['y']))\n",
    "\n",
    "\n",
    "##########################################\n",
    "# Huper parameter tuning\n",
    "##########################################\n",
    "# model accuracy for X_test\n",
    "mse= mean_squared_error(preds, data['test']['y'])\n",
    "run.log('Accuracy', np.float(mse))\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "# AML content - start\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "# Log mse in Azure ML logs\n",
    "run.log('mse',  np.float(mse))\n",
    "\n",
    "# Save the model to the outputs directory for capture\n",
    "model_file = 'diabetes_helen.pkl'\n",
    "model_file_name=os.path.join(output_dir, model_file)\n",
    "joblib.dump(value = reg, filename = model_file_name);\n",
    "print(run.get_file_names())\n",
    "\n",
    "# upload the model file explicitly into artifacts Azure ML artifacts\n",
    "run.upload_file(name = model_file_name, path_or_stream = model_file_name)\n",
    "\n",
    "# register model in Azure ML Resitry \n",
    "model = run.register_model(model_name='helen_test',model_path=model_file_name)\n",
    "print(model.name, model.id, model.version, sep='\\t')\n",
    "\n",
    "for a in range (len(preds)):\n",
    "    run.log_row(\"Error: Estimate  - Actual\", x=a, y=abs (float (preds[a]) - float(y_test[a])))\n",
    "    \n",
    "\n",
    "# Creating file to oputput\n",
    "num_rows, num_cols = X_test.shape\n",
    "pred = preds.reshape((num_rows, 1))\n",
    "actual=y_test.reshape((num_rows, 1))\n",
    "\n",
    "tmp_npy = np.append (X_test, actual, 1)\n",
    "helen_numpy = np.append (tmp_npy, pred, 1)\n",
    "print ('helen_numpy shape ',helen_numpy.shape)\n",
    "\n",
    "helen_pandas=pd.DataFrame(data=helen_numpy)\n",
    "\n",
    "LOCALFILENAME='helen_score_file.txt'\n",
    "score_dir='./logs'\n",
    "score_dir='./helen/score'\n",
    "\n",
    "# Uploading file as articraft\n",
    "os.makedirs (score_dir,exist_ok=True)\n",
    "score_file = os.path.join(score_dir, LOCALFILENAME) \n",
    "helen_pandas.to_csv(score_file, sep=',', encoding='utf-8', index=False)\n",
    "print ('file name', score_file)\n",
    "\n",
    "# upload scored data explicitly into artifacts \n",
    "run.upload_file(name = score_file, path_or_stream = score_file)\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "# Plots\n",
    "##########################################\n",
    "\n",
    "    \n",
    "# Logging histogram plot in Azue ML \n",
    "num_rows, num_cols = X_test.shape\n",
    "pred = preds.reshape((num_rows, 1))\n",
    "actual=y_test.reshape((num_rows, 1))\n",
    "tmp_npy = np.append (X_test, actual, 1)\n",
    "helen_numpy = np.append (tmp_npy, pred, 1)\n",
    "\n",
    "\n",
    "f=helen_numpy\n",
    "print (f.shape)\n",
    "fnrow=f.shape[0]\n",
    "fncol=f.shape[1]\n",
    "print ( \" rows \", fnrow, \"columns \", fncol)\n",
    "\n",
    "# Histograms to all columns\n",
    "i=0\n",
    "for i in range (fncol):\n",
    "    title= str (i) + ' nr column  '\n",
    "    plot.title(title)\n",
    "    plot.hist (f[:,[i]],bins=30,color='blue',edgecolor='white')\n",
    "    #CORRECTplot.show()\n",
    "    run.log_image ('Helen plot_' + str (i),plot=plot)\n",
    "    plot.clf()\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "# AML content - end\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "# create output refernce for dataset in pipeline step\n",
    "##########################################\n",
    "\n",
    "#mounted_output_path = os.environ['AZUREML_DATAREFERENCE_diabetes_temp_ds']\n",
    "#os.makedirs(mounted_output_path, exist_ok=True)\n",
    "#score_file = os.path.join(mounted_output_path, LOCALFILENAME) \n",
    "#helen_pandas.to_csv(score_file, sep=',', encoding='utf-8', index=False)\n",
    "#print ('file name to somewhere', score_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for AML run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sklearn_conda_dependencies.yml\n",
    "\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- matplotlib\n",
    "- pip:\n",
    "  - azureml-defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create env from conda dependencies\n",
    "\n",
    "from azureml.core import Environment\n",
    "\n",
    "try: \n",
    "    sklearn_env =Environment.get(workspace=ws,name=\"sklearn-env\")\n",
    "    print ('environment exists ')\n",
    "    sklearn_env\n",
    "except: \n",
    "    sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = './sklearn_conda_dependencies.yml')\n",
    "    sklearn_env.docker.enabled = True\n",
    "    sklearn_env.python.user_managed_dependencies = False\n",
    "    sklearn_env.register(workspace = ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create env from earlier saved environment\n",
    "#sklearn_env =Environment.get(workspace=ws,name=\"sklearn-env\")\n",
    "sklearn_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Accessing dataset which is already registered\n",
    "# get dataset by dataset name\n",
    "\n",
    "diabetes = Dataset.get_by_name(workspace=ws, name='diabetes')\n",
    "\n",
    "d_data = diabetes.to_pandas_dataframe()\n",
    "d_data.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scriptrunconfig - configuring the execution for remote and local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE TEST\n",
    "#####################\n",
    "# SHORT test version \n",
    "####################\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "script_dir = script_dir\n",
    "script_file = 'diabetes2021_test.py'\n",
    "\n",
    "\n",
    "experiment = Experiment(workspace=ws, name=\"diabetes2021_env\")\n",
    "\n",
    "est = ScriptRunConfig(source_directory=script_dir,\n",
    "                      script=script_file,\n",
    "                      arguments=['--kernel', 'linear', '--penalty', 1.0,'--ds', diabetes.as_named_input('diabetes')]\n",
    "                     )\n",
    "\n",
    "# Submit the estimator as part of your experiment run\n",
    "est.run_config.target=compute_target\n",
    "\n",
    "# Correct: Submit Local\n",
    "est.run_config.target='local'\n",
    "est.run_config.environment=sklearn_env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE TEST\n",
    "###########################\n",
    "# THIS FULL AND COMPLETE\n",
    "###########################\n",
    "\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "script_dir = script_dir\n",
    "script_file = 'diabetes2021_prep.py'\n",
    "# CORRECT script_file= 'diabetes2021_test.py'\n",
    "\n",
    "\n",
    "experiment = Experiment(workspace=ws, name=\"diabetes2021_env\")\n",
    "\n",
    "est = ScriptRunConfig(source_directory=script_dir,\n",
    "                      script=script_file,\n",
    "                      arguments=['--kernel', 'linear', '--ridge', 0.03,'--ds', diabetes.as_named_input('diabetes')]\n",
    "                      #outputs=[diabetes_scored]\n",
    "                      )\n",
    "\n",
    "# Submit the estimator as part of your experiment run\n",
    "est.run_config.target=compute_target\n",
    "\n",
    "# Correct: Submit Local\n",
    "est.run_config.target='local'\n",
    "est.run_config.environment=sklearn_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run - remote or local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single run\n",
    "experiment_run = experiment.submit(est)\n",
    "\n",
    "RunDetails(experiment_run).show()\n",
    "\n",
    "\n",
    "experiment_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning run - remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\n",
    "    \n",
    "\n",
    "param_sampling = RandomParameterSampling( {\n",
    "    \"--kernel\": choice('linear', 'rbf'),\n",
    "    \"--ridge\": choice(0.01, 0.03, 0.05)\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "hyperdrive_config = HyperDriveConfig(run_config=est,\n",
    "                                     hyperparameter_sampling=param_sampling, \n",
    "                                     primary_metric_name='mse',\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MINIMIZE,\n",
    "                                     max_total_runs=12,\n",
    "                                     max_concurrent_runs=4)\n",
    "\n",
    "# start the HyperDrive run\n",
    "hyperdrive_run = experiment.submit(hyperdrive_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving results of hypermarameter run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "print(best_run.get_details()['runDefinition']['arguments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= hyperdrive_run.get_children_sorted_by_primary_metric()\n",
    "for aa in a:\n",
    "    print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in best_run.get_file_names():\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='diabates_hyper', model_path='helen/output/diabetes_helen.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import parser\n",
    "from azureml.core import Dataset, Run\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.externals import joblib\n",
    "import joblib\n",
    "from matplotlib import pyplot as plot\n",
    "from azureml.core import Workspace, Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching model from Azure ML ws\n",
    "\n",
    "import os\n",
    "modelname='diabates_hyper'\n",
    "model_file= \"diabetes_helen.pkl\"\n",
    "\n",
    "model = Model(ws, modelname)\n",
    "\n",
    "output_dir='helen'\n",
    "os.makedirs (output_dir,exist_ok=True)\n",
    "\n",
    "#model = Model(ws, modelname, version=4)\n",
    "model.download(target_dir=output_dir, exist_ok=True)\n",
    "print (model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring - i have here issue with sklearn versions\n",
    "\n",
    "import joblib as joblib\n",
    "model_file_name = os.path.join(output_dir, model_file)\n",
    "# checking file exists\n",
    "os.stat(model_file_name)\n",
    "\n",
    "# ready for scoring\n",
    "my_model = joblib.load(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parser\n",
    "from azureml.core import Dataset, Run\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import joblib\n",
    "from matplotlib import pyplot as plot\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "# load the TabularDataset or any dataset to pandas DataFrame \n",
    "df = diabetes.to_pandas_dataframe()\n",
    "\n",
    "\n",
    "dd_data=df\n",
    "dd_data=dd_data.drop(columns=[\"Target\"])\n",
    "x_array=dd_data.to_numpy()\n",
    "print (\"correct x !!!! \", type (dd_data))\n",
    "\n",
    "\n",
    "dd_target=df\n",
    "dd_target=dd_target[[\"Target\"]]\n",
    "y_array=dd_target.to_numpy()\n",
    "print (\"correct y !!!! \", type (dd_data))\n",
    "\n",
    "# My regural python code\n",
    "y=y_array\n",
    "X=x_array\n",
    "columns = ['age', 'gender', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "\n",
    "\n",
    "data = {\n",
    "   \"test\":{\"X\": X, \"y\": y}\n",
    "}\n",
    "preds = my_model.predict(data['test']['X'])\n",
    "mse= mean_squared_error(preds, data['test']['y']) \n",
    "print ('mse = ', mse)\n",
    "\n",
    "\n",
    "\n",
    "###########################\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = preds\n",
    "y = y\n",
    "#plt.plot(x,y)\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"predicted\")\n",
    "plt.ylabel(\"actual\")\n",
    "plt.title('Predicted vs Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for environemnt\n",
    "from azureml.core import Environment\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "# List Environments and packages in my workspace\n",
    "for env in envs:\n",
    "    if env.startswith(\"\"):\n",
    "    #if env.startswith(\"sk\"):\n",
    "        print(\"Name\",env)\n",
    "        print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n",
    "        \n",
    "# Use curated environment from AML named \"AzureML-Tutorial\"\n",
    "\n",
    "# Correct curated_environment = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\n",
    "# Correct Custom environment: Environment.get(workspace=ws,name=\"myenv\",version=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
