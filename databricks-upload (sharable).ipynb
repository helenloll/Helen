{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core.runconfig import JarLibrary\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import DatabricksStep\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '8b2f4e94-e7b0-42e5-b775-dd2d5968c4e6'\n",
    "resource_group  = 'HelenMachineLearning'\n",
    "workspace_name  = 'HelenMachineLearning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.get(\n",
    "    name=workspace_name,\n",
    "    subscription_id=subscription_id,\n",
    "    resource_group=resource_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your account info before running.\n",
    " \n",
    "db_compute_name=os.getenv(\"DATABRICKS_COMPUTE_NAME\", \"databricks\") # Databricks compute name\n",
    "db_resource_group=os.getenv(\"DATABRICKS_RESOURCE_GROUP\", \"Databricks2019\") # Databricks resource group\n",
    "db_workspace_name=os.getenv(\"DATABRICKS_WORKSPACE_NAME\", \"Databricks2019\") # Databricks workspace name\n",
    "db_access_token=os.getenv(\"DATABRICKS_ACCESS_TOKEN\", \"dapi820ce3b8cd06d9330c863c63\") # Databricks access token\n",
    " \n",
    "try:\n",
    "    databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\n",
    "    print('Compute target {} already exists'.format(db_compute_name))\n",
    "except ComputeTargetException:\n",
    "    print('Compute not found, will use below parameters to attach new one')\n",
    "    print('db_compute_name {}'.format(db_compute_name))\n",
    "    print('db_resource_group {}'.format(db_resource_group))\n",
    "    print('db_workspace_name {}'.format(db_workspace_name))\n",
    "    print('db_access_token {}'.format(db_access_token))\n",
    " \n",
    "    config = DatabricksCompute.attach_configuration(\n",
    "        resource_group = db_resource_group,\n",
    "        workspace_name = db_workspace_name,\n",
    "        access_token= db_access_token)\n",
    "    databricks_compute=ComputeTarget.attach(ws, db_compute_name, config)\n",
    "    databricks_compute.wait_for_completion(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One time setup: Install databricks CLI and configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: databricks-cli in c:\\users\\akvasude\\appdata\\local\\continuum\\miniconda3\\envs\\cli_dev\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\akvasude\\appdata\\local\\continuum\\miniconda3\\envs\\cli_dev\\lib\\site-packages (from databricks-cli) (0.8.2)\n",
      "Requirement already satisfied: click>=6.7 in c:\\users\\akvasude\\appdata\\local\\continuum\\miniconda3\\envs\\cli_dev\\lib\\site-packages (from databricks-cli) (6.7)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\akvasude\\appdata\\local\\continuum\\miniconda3\\envs\\cli_dev\\lib\\site-packages (from databricks-cli) (1.11.0)\n",
      "Requirement already satisfied: requests>=2.17.3 in c:\\users\\akvasude\\appdata\\local\\continuum\\miniconda3\\envs\\cli_dev\\lib\\site-packages (from databricks-cli) (2.19.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akvasude\\appdata\\local\\continuum\\miniconda3\\envs\\cli_dev\\lib\\site-packages (from requests>=2.17.3->databricks-cli) (2018.11.29)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\akvasude\\appdata\\local\\continuum\\miniconda3\\envs\\cli_dev\\lib\\site-packages (from requests>=2.17.3->databricks-cli) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\users\\akvasude\\appdata\\local\\continuum\\miniconda3\\envs\\cli_dev\\lib\\site-packages (from requests>=2.17.3->databricks-cli) (2.7)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in c:\\users\\akvasude\\appdata\\local\\continuum\\miniconda3\\envs\\cli_dev\\lib\\site-packages (from requests>=2.17.3->databricks-cli) (1.23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "msftkube 1.0.1598662 has requirement applicationinsights==0.11.5, but you'll have applicationinsights 0.11.9 which is incompatible.\n",
      "msftkube 1.0.1598662 has requirement urllib3==1.22, but you'll have urllib3 1.23 which is incompatible.\n",
      "azureml-sdk 0.1.0.1060109 has requirement azureml-core==0.1.0.1060109, but you'll have azureml-core 0.1.0.0 which is incompatible.\n",
      "azureml-sdk 0.1.0.1060109 has requirement azureml-train==0.1.0.1060109, but you'll have azureml-train 0.1.0.0 which is incompatible.\n",
      "azureml-requirements 0.1.0.888002 has requirement azureml-core==0.1.0.888002, but you'll have azureml-core 0.1.0.0 which is incompatible.\n",
      "azureml-requirements 0.1.0.888002 has requirement azureml-train==0.1.0.888002, but you'll have azureml-train 0.1.0.0 which is incompatible.\n",
      "azureml-contrib-widgets 0.1.0.1060109 has requirement azureml-core==0.1.0.1060109, but you'll have azureml-core 0.1.0.0 which is incompatible.\n",
      "azureml-contrib-widgets 0.1.0.1060109 has requirement azureml-train-core==0.1.0.1060109, but you'll have azureml-train-core 0.1.0.0 which is incompatible.\n",
      "azureml-contrib-tensorboard 0.1.0.1060109 has requirement azureml-core==0.1.0.1060109, but you'll have azureml-core 0.1.0.0 which is incompatible.\n",
      "azure-datalake-store 0.0.19 has requirement msrest~=0.4.5, but you'll have msrest 0.5.4 which is incompatible.\n",
      "azure-cli-appservice 0.1.32 has requirement azure-mgmt-containerregistry==2.0.0, but you'll have azure-mgmt-containerregistry 2.8.0 which is incompatible.\n",
      "azure-cli-acr 2.0.24 has requirement azure-mgmt-containerregistry==2.0.0, but you'll have azure-mgmt-containerregistry 2.8.0 which is incompatible.\n",
      "azure-cli-ml 0.1.0.0 has requirement docker>=3.7.2, but you'll have docker 3.3.0 which is incompatible.\n",
      "azure-cli-ml 0.1.0.0 has requirement msrest>=0.6.6, but you'll have msrest 0.5.4 which is incompatible.\n",
      "azure-cli-ml 0.1.0.0 has requirement pyyaml>=5.1.0, but you'll have pyyaml 4.2b4 which is incompatible.\n",
      "azure-cli-ml 0.1.0.0 has requirement requests>=2.21.0, but you'll have requests 2.19.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install databricks-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure CLI\n",
    "\n",
    "> Run `dbfs configure --token` at command line to setup authentication. You'll need to specify Databricks URL and Personal Access Token. This link has details: https://docs.databricks.com/dev-tools/cli/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload files from local computer to DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \".\\code\"\n",
    "dbfs_path = \"dbfs:/data/UploadTest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you need to delete existing DBFS folder\n",
    "# !dbfs rm -r {dbfs_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\code\\hello.py -> dbfs:/data/UploadTest/hello.py\n"
     ]
    }
   ],
   "source": [
    "!dbfs cp -r {local_path} {dbfs_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the file that was uploaded to DBFS in DatabricksStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_script_path = dbfs_path + \"/hello.py\"  # append name of the entry script\n",
    "print(python_script_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbPythonInDbfsStep = DatabricksStep(\n",
    "    name=\"DBPythonInDBFS\",\n",
    "    run_name='DB_Python_demo',\n",
    "    compute_target=databricks_compute,\n",
    "    python_script_path=python_script_path,\n",
    "    python_script_params={'arg1', 'arg2'},\n",
    "    num_workers=2,\n",
    "    allow_reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step DBPythonInDBFS [d46aac4b][ed63aea8-fe76-4e4b-a556-95a4e17e1506], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun c1b4d9f2-0e26-4e1a-8972-07721acc76d2\n",
      "Link to Azure Machine Learning studio: https://ml.azure.com/experiments/DB_Python_demo/runs/c1b4d9f2-0e26-4e1a-8972-07721acc76d2?wsid=/subscriptions/8b2f4e94-e7b0-42e5-b775-dd2d5968c4e6/resourcegroups/HelenMachineLearning/workspaces/HelenMachineLearning\n",
      "PipelineRunId: c1b4d9f2-0e26-4e1a-8972-07721acc76d2\n",
      "Link to Portal: https://ml.azure.com/experiments/DB_Python_demo/runs/c1b4d9f2-0e26-4e1a-8972-07721acc76d2?wsid=/subscriptions/8b2f4e94-e7b0-42e5-b775-dd2d5968c4e6/resourcegroups/HelenMachineLearning/workspaces/HelenMachineLearning\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: ad632b88-ef78-4ba6-92ac-11aee0b27d41\n",
      "Link to Portal: https://ml.azure.com/experiments/DB_Python_demo/runs/ad632b88-ef78-4ba6-92ac-11aee0b27d41?wsid=/subscriptions/8b2f4e94-e7b0-42e5-b775-dd2d5968c4e6/resourcegroups/HelenMachineLearning/workspaces/HelenMachineLearning\n",
      "StepRun( DBPythonInDBFS ) Status: Running\n"
     ]
    }
   ],
   "source": [
    "steps = [dbPythonInDbfsStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'DB_Python_demo').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
